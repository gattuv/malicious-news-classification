\chapter{Related Work}

The current approaches to validating news and facts and filtering out unreliable content are currently limited, but there is a good foundation for future systems to build on.  These approaches either attempt to verify content independently, or use prior knowledge and biases to block out suspicious sources completely, usually after user complaints.  For example, organizations like Snopes and PolitiFact employ professionals to verify claims manually, and universities usually post guidelines for their students on how to recognize trustworthy sources of information.  Even Facebook turned to third-party fact-checking organizations, including Snopes and PolitiFact, to flag disputed stories in an attempt to stem the tide of a viral fake news story \cite{fighingFakeNews}.  However, given the speed at which sensational news travels, waiting on journalists to perform thorough research is not entirely feasible and the reader must do their own due diligence.


\section{Manual Fact Checking and Source Validation}

Snopes and PolitiFact are two prominent and well-respected fact checking organizations that regularly post fact checks on controversial issues.  Snopes.com is a completely independent and self-sufficient website that was launched in 1994 by David Mikkelson to host his research on the prevalent urban legends of his time.  Since then, Snopes has since grown into a resource that many people, including journalists, rely on for credible information \cite{snopesAbout}.  PolitiFact is much more recent product and focuses primarily on fact-checking elected officials and others in the public eye who speak about politics.  PolitiFact is run by editors and reporters from the \textit{Tampa Bay Times}, an independent newspaper, and is owned by the non-for-profit Poynter Institute, a school for journalism in Florida \cite{politifactAbout}.  Though both organizations share a similar mission of informing their readers, PolitiFact chooses to only analyze the most newsworthy political claims while Snopes chooses whichever topics their readers are most interested in at any given time.

For a more immediate sense on the veracity of an article, readers can either dig into the claims made in the article themselves, or use basic guidelines to be decide how wary or suspicious they should be of the publishing source.  One relevant resource cited by Harvard Library's Research Guides is OpenSources.co, an ongoing project that retains an updated registry of misleading and fake news \cite{harvardResearch}.  Led by Dr. Melissa Zimdars at Merrimack College, the OpenSources team analyzes each source to determine the lack of transparency, level of inaccuracy, extreme bias, and other indications of purposeful misinformation.  Some of the labels they use to tag untrustworthy sources are "fake" for sources that fabricate stories, "bias" for sources that have an extreme political view and often decontextualize information, "hate" for sources that actively promote racism, misogyny, homophobia and other forms of discrimination.  The labels they use to tag potentially trustworthy sources are "political" for sources that provide verifiable information for certain political views, and "reliable" for sources that post information "in a manner consistent with traditional and ethical practices in journalism" \cite{opensources}.

OpenSources' steps for labelling include analysis of the title and domain, publishing author and sources cited (if any), writing style, aesthetics of the web page, and the source's social media presence.  For example, if the title or domain seems like an imitation of a more prominent news authority, or the web page seems like a misshapen blog, it is highly likely that the website is not a credible source of reliable information.  Furthermore, if the writing style seems inconsistent, is grammatically incorrect, or contains a lot of capital words with exclamation marks, the article may be clickbait and purposefully incendiary to invoke extreme feelings in the reader \cite{opensources}.

For an automated system to emulate the manual process that OpenSources contributors use for source tagging, the system would need to detect all the red flags a human would look out for.  However, since some of these markers are not easily quantifiable, this logic is not very formulaic and hinges on the discretion of the arbitrator, whether human or machine.  To overcome possible ambiguities, some systems, like browser extensions, just blacklist whole domains.  Other systems, like the one introduced in the following section, iteratively learns whether or not a source should be trusted.


\section{Knowledge Fusion}

In 2014, a research team at Google attempted to build a system capable of discerning truth from an aggregate of information scraped off of the web \cite{knowledgeFusion}.  They structured their study to tackle their newfound problem of identifying the probability that a scraped subject-predicate-object triple of information is actually true.  This challenge, which they called the knowledge fusion problem, builds on the data fusion problem which is the challenge of identifying the true values of data given a pool of conflicting information (e.g., determining President Obama's birthplace from a set of possible places extracted from different in articles, blogs, and editorials).  The motivation for the knowledge fusion problem is to use these true-labelled triples to build a web-scale knowledge base for fast and reliable information retrieval, similar to the knowledge graph used by the Google search engine.

Classical approaches to the data fusion problem predominantly determined truth values in accordance with either a voting scheme, rule-based system, a trustworthiness metric for quality-based labelling, or a source relationship-based method \cite{trustworthiness}.  The early rule-based systems updated truth values by mirroring the most recent source's value, or taking minimums, maximums, or averages for numerical values.  Quality-based approaches either rely on external metrics like page ranks and similarity scores, or compute likelihoods of correctness for the data.  The relation-based approaches try to establish links between sources by either attributing derivative sources with lower weights, or clustering sources into subsets to act as supporting evidence for particular data values.

The authors of "From Data Fusion to Knowledge Fusion" \cite{knowledgeFusion} experimented with adaptations to three data fusion techniques.  The first technique was a simple vote-based algorithm called VOTE that served as their baseline.  The other two techniques were statistical algorithms that learned sources' trustworthiness using the source's historical accuracy rate.  The trustworthiness of each source was then used to model the data values as a posterior distribution conditioned on the distribution of the data.  In each posterior distribution, the trustworthiness of the source serves as the likelihood of the value, and the distribution of the values serves as the prior probability \cite{popaccu}.

In the first statistical technique, called ACCU, a simplifying assumption is made that greatly reduces the complexity of the determining the prior probability; specifically, there are a constant number of uniformly distributed false values.  In contrast, the second technique, called POPACCU, derives the prior probability distribution of values using the observed data.  The authors chose POPACCU because it is more robust than ACCU in scenarios where sources copy each other \cite{popaccu}.  With enough false copycat sources, the distribution of false values becomes quite skewed and far from uniform.  To tackle the knowledge fusion problem, these data fusion techniques are modified to produce truthfulness probabilities as opposed to binary output: the ACCU techniques are modified to output their posterior probability, and the voting-based approach is modified to output the proportion of sources that agreed with the most popular value.

To evaluate their techniques, they use verified triples on Freebase, an open database of world knowledge \cite{freebase}, as their gold standard.  Triples of subject-predicate-object form (s-p-o) contained in Freebase are considered as always true, and s-p-o triples that are not in Freebase as a complete triple, but are seen as s-p pairs, are labelled false under the local closed-world assumption (LCWA).  In a closed-world system, all information is known by the system; however, in a LCWA system, the system only has complete knowledge about any local knowledge it contains.  Thus, if Freebase has knowledge of a particular data item (the s-p pair), under the LCWA assumption it has complete knowledge about that topic and so any triple formed using that subject-predicate is untrue if the triple is not already contained in Freebase.  Triples made of subject-predicate pairs that are not present in Freebase are ambiguous are simply excluded from the experiment \cite{knowledgeFusion}.

To numerically evaluate the performance of each technique, a precision versus recall plot is constructed.  The precision and recall are recorded as each new triple is extracted from a data source, and the area under this precision versus recall curve (AUC-PR) is used to describe the performance of each technique.  The results of their study show that the ACCU models have a better AUC-PR than VOTE.  The results also show that the VOTE technique usually underestimates the probability of true values.  That is, in scenarios where there are very few sources of data values, or none of the sources agree on any particular value, the confidence, or prediction probability, of the predicted value is quite low for VOTE.  This result stems from the fact that the VOTE technique does not learn to designate sources as trustworthy, and so when there are very few corroborating triples, VOTE has a low prediction confidence.  It turns out that ACCU has the highest AUC-PR, but slightly higher deviation between real probability and prediction probability as triples are iteratively added into consideration.

The results of this paper show that it is possible to make a fact-assessing system with high precision-recall if the system has immediate access to enough data from trustworthy sources.  However, this luxury is not available to any system that would wish to moderate news articles in realtime.  While the system in this study is far better solution than blacklisting whole domains, it may still develop a bias and excessively filter out articles.  For example, the system may be unfair to newer authors at organizations that have developed untrustworthy priors due to a few select publications.  This system may even unfairly block content from newer, unrecognized sources when they report breaking news that has not yet been corroborated.

Today, readers have almost a limitless number of sources for information - building a system that remembers the trustworthiness of each source, or tries to determine the source's trustworthiness based information previously published, if any, is infeasible.  A more feasible approach does not try to determine whether or not data is true, but instead tries to determine if the data is probably untrue based on how it is presented.

